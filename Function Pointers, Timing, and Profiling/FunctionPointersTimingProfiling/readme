Name: Karl Chavez

I certify that I completed all of the work myself with no aid from anyone else aside from the instructor or the undergraduate graders.

Part 1:

Q1: The structure I implemented was stack. The reason being is because it was much easier to do insertion and deletion on a stack for both queue and array structures since change only occured on one end. With queue, however, you have to worry about the start and the end of the structure when you are doing insertion and deletion. 

Q2: One advantage of the linked list approach was that it created little space when inserting an element into the linked list. The memory needed to create a new node was just an integer and a node pointer (and other things but are more complex). However, one disadvantage of the linked list approach was dealing with pointers. Keeping track of where pointers are pointing to is more cumbersome than just having an "array" of elements. One advantage of the array and table doubling approach is that it was easy to keep track of insertion and deletions. Since this was an "array" structure-like, it was much easier to insert/delete elements in a number of contigous bytes and the elements contained can be accessed exactly like those in a statically allocated array. However, one disadvantage of the array and table doubling approach was how much memory it took up whenever a changed occured in the "array". For example, if the "array" reached a quarter of its current max size or it was full, the creation of an "array" was needed. If we were dealing with thousands or millions of entries, a gigantic amount of arrays will be created throughout the procedure. Not to mention the amount of copying that needs to happen whenever a new "array" is created. Copying from one array to another can take a very long time if the entries are big.

Q3: I believe the linked list approach will be must faster. The reason being is because the array and table doubling approach constantly has to copy elements from one "array" to "another" in deletion/insertions. For a linked list approach, whenver an insertion occurs, it just mallocs data for the new node and connects it to the linked list through pointers. However, for the array and table doubling approach, if the "array" reaches a quarter of the max size or when it's full, it needs to create a new array AND copy the elements over. The linked list approach has pointers to the first and last node so it doesn't have to go through all the elements when inserting and deleting elements. For that reason, I think the linked list approach will be faster. 

Q4: There are a few reasons to use void pointers, but in this lab the reason being is because pointers to void can be used to pass "typeless" parameters to functions, and then the functions can use a cast with the pointer to access data which is to be interpreted in a certain way. Meaning, we don't have to pass the whole data structure to a function because that will be a waste of memory and time. Instead, we can just pass a pointer to the data structure which will cost less memory and will be more efficient.

Part 2:

Q1: It is possible that the CPU time that we calculate from the methods created isn't actually completely correct because different computers and operating systems vary wildly in how they keep track of CPU time. Meaning, somebody who has the same exact code but has a completely different specs and operating system can have a diffrent CPU time. Different computers can output better or worse results. 

Q2: The obvious (but rather silly) way to reduce the CPU time is to write less code. A file that outputs one random number will have a much smaller CPU time than a file that runs minecraft. The file that runs minecraft will obviously have more code written than something that simply outputs one random number.However, both files have different utilizations compared to each other.

Q3: CPU time isn't a good measure of the performance of a program. While it is good to check the CPU time of a program, even more importantly is the context of the program. Meaning, what is the functionaly of the program and what does it accomplish? A code can run lightening speeds, but only accomplishes the basic task of outputting "Hello World!" while another code can take hours to run, but generates millions of data information for all the employees and residentials in a hospital. It is important to consider the context and the CPU time to have a good measure of the performance of a certain program. 

Q4: After seeing the results for the test, Stack1 with the linked list structure was much faster than Stack2 with the array and table doubling structure. I was right with my prediction. The reason being is because Adding or removing elements is a lot faster in a linked list than in an array. In a linked list, there are pointers to the front and the end of the list so it's faster to add or remove elements. Furthermore, the array waste so much memory and time allocating an array and copying the elements from the old to the new array. I tested this with 40,000,000 (40 million) entries. The time for Stack1 was 6 seconds and the time for Stack2 was 12 seconds which was double than Stack1's time.  

Q5: After recompiling my sources, the time was cut in half with the "-O" flag compared to running the makefile without the "-O" flag. That's because we put an optimization level when compiling our code so it ran it faster. Without the "-O" flag, the time was 12 seconds for Stack1 and 6 seconds for Stack2. Using the "-O" flag cut the time in half with Stack1 running in 6 seconds and Stack2 in 3 seconds. 

Q6: After recompiling my sources with the added -O3 flag, the time increased slightly but not by much. Since we increased the optimization level even more, the compile time increased. "-O3" is a step further in the compiler's optimization level which is why we see the slight increase in CPU time.

Q7: There is a difference. Q5's file size executable is slightly bigger than Q4's file size. The reason being is using a higher optimization level for performance will increase the file size. Since we used a higher optimization level for Q5 than Q4, we should expect an increase in file size. 

Q8: When we increase the optimization level, it takes more time and memory for a large functions implemented in Stack1 and Stack2. When we increase the optimization level, the compiler tries to reduce code size and execution time, without performing any optimizations that take a great deal of compilation time. Like us humans, if we want a more optimized solution for a certain problem, then we need more time to think about it. We need more time thinking of a better and faster solution. This is the same as the compiler. 

Q9: We have so many different variant types of the "-O" flags for different optimization goals. Optimizing for a certain goal has an impact on other goals. Each different levels of optimization has its own trade-offs. The best optimization level for an application depends on the application and optimization goal. 

Part 3:

Q1: I was predicting that push for both structures would take up most of the time, but it was only for Stack2 which is the array and table doubling structure. For Stack1, the function that took the most time was "etext" and the second was "popS1"; I was surprised to see that push wasn't even on the list so I though that was weird. I am not entirely sure what "etext" even means. 

Q2: For Stack1 structure, the function that took up the most time is "etext". The second function that took up the most time is "popS1". For Stack2 structure, the function that took up the most time is "pushS2". 

Q3: If we were given the option to create our own functions and implementations, then I think it's definitely possible to shrink the time spent on each of the functions. However, assuming I followed what the lab has told me, I've implemented each function to its fastest implementation. I was having a hard time freeing the Stack structures that I created in the push and pop functions so I wasn't able to free them at all. The time spent on each function would've shrank if I could free the variables because there would be less memory in the system. 

Q4: I think profiling does suffer from the same shortcomings that timing suffers, but profiling gives us more context than timing does. Timing gives us a CPU time number which doesn't really explain anything other than the time it took for the file to run. With profiling, however, we can divide our attention to each individual function and how much time they ran. This gives us context which implementations was taking up more time than others. Profiling gives us a much wider context of the program as we are able to find out how much time each function took to run, how many times it was called, its subroutines, and overall how it compares to other functions implemented. However, I still believe that more context is needed to fully grasp how fast a program runs. We would need more information about the interworkings of each function. 

Q5: My favorite flag would have to be "-b". I love it because it doesn't print the verbose blurbs that try to explain the meaning of all of fields in the tables. This flag is useful if you just want to see the output and not have to see a ton of blurbs on the screen. Having a much cleaner, less cluttered screen is always good for everyone and doesn't make you go insane. 

